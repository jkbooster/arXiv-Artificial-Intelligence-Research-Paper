# -*- coding: utf-8 -*-
"""website_scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FQKH1W_ugsKzv8vhqkylMZBT2x4daz88
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import pandas as pd
from PyPDF2 import PdfReader
import tiktoken

def scrape_arXiv(limit = 10):

  base_url = "https://arxiv.org/list/cs.AI/pastweek?show=100"

  response = requests.get(base_url)
  soup = BeautifulSoup(response.content, "html.parser")

  title = soup.title.string.strip()

  pdf_links = []
  max = 10
  pdf_links = pdf_links[:max]

  print(title + " Research Papers")
  for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"]
        if href.startswith("/pdf/"):
            full_link = urljoin(base_url, href + ".pdf")
            pdf_links.append(full_link)
        if len(pdf_links) >= limit:
            break
  return pdf_links

pdfs = scrape_arXiv(limit = 10)
for link in pdfs:
    print(link)

df = pd.DataFrame(pdfs, columns=["pdf_link"])
df.to_csv("arxiv_pdfs.csv", index=False)

import pandas as pd

df_existing = pd.read_csv("arxiv_pdfs.csv")
existing_links = set(df_existing["pdf_link"])

new_links = [...]
unique_new = [link for link in new_links if link not in existing_links]

if unique_new:
    new_df = pd.DataFrame({"pdf_link": unique_new})
    df_combined = pd.concat([df_existing, new_df], ignore_index=True)
    df_combined.to_csv("arxiv_pdf_links.csv", index=False)

import pandas as pd
import requests
from io import BytesIO
from PyPDF2 import PdfReader
from groq import Groq

client = Groq(api_key="your_grokai_api_key")

def extract_text(pdf_url):
    try:
        response = requests.get(pdf_url, timeout=10)
        pdf_file = BytesIO(response.content)
        reader = PdfReader(pdf_file)
        return " ".join(page.extract_text() or "" for page in reader.pages[:3])
    except Exception as e:
        print(f"Failed: {e}")
        return ""

def summarize(text):
    prompt = f"Summarize this academic research paper:\n\n{text[:3000]}"
    res = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content.strip()

df = pd.read_csv("arxiv_pdf_links.csv")
summaries = []

for url in df["pdf_link"]:
    text = extract_text(url)
    if text:
        summary = summarize(text)
        summaries.append({"pdf_link": url, "summary": summary})
        print(f"Summarized: {url}")
    else:
        print(f"Skipped: {url}")

pd.DataFrame(summaries).to_csv("arxiv_summaries.csv", index=False)

from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.pagesizes import LETTER
from reportlab.lib.styles import getSampleStyleSheet

df = pd.read_csv("arxiv_summaries.csv")

doc = SimpleDocTemplate("arxiv_summaries.pdf", pagesize=LETTER)
styles = getSampleStyleSheet()
flowables = []

for _, row in df.iterrows():
    flowables.append(Paragraph(f"<b>Paper:</b> {row['pdf_link']}", styles['Heading4']))
    flowables.append(Paragraph(f"<b>Summary:</b> {row['summary']}", styles['BodyText']))
    flowables.append(Spacer(1, 12))

doc.build(flowables)

from google.colab import files
files.download('arxiv_summaries.pdf')



